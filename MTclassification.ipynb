{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MTclassification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3QjS4ryuMyu"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install --upgrade nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWApj7UTvSI1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpAQCpmA8EzZ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaTDO6fEvQrY"
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.nist_score import sentence_nist\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model\n",
        "from sklearn import neural_network\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LBEbQxzZ7lE"
      },
      "source": [
        "def process(words):\n",
        "  words = words.replace(\"<br />\", \" \").rstrip()\n",
        "  temp = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = words.translate(temp)\n",
        "  # words = words.lower()\n",
        "  words = words.split()\n",
        "  words = [word for word in words if word not in stopwords]\n",
        "  processed_words = ' '.join(words)\n",
        "  return processed_words\n",
        "\n",
        "def strip_punc(s):\n",
        "  return s.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def set_stopwords():\n",
        "  stopwords = open(\"/content/drive/My Drive/translations/stopwords.en.txt\", \"r\", encoding=\"utf8\")\n",
        "  words = stopwords.read().split(\"\\n\")\n",
        "  stopwords.close()\n",
        "  return words"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFGuo7hg8tS1"
      },
      "source": [
        "def build_matrix(cols,data):\n",
        "  n = data.shape[0]\n",
        "  X = np.zeros((n,1))\n",
        "  vec_cols = []\n",
        "  for col in cols:\n",
        "    if col in vec_cols:\n",
        "      array = np.asarray(list(data[col]))\n",
        "      X = np.c_[ X, array ] \n",
        "    else:\n",
        "      array = np.asarray(data[col])\n",
        "      array = array.reshape(array.shape[0],-1)\n",
        "      X = np.c_[ X, array ] \n",
        "  return X[:,1:]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0AAWklW9mB8"
      },
      "source": [
        "def cosine_similarity(A, B):\n",
        "  return (np.dot(A,B))/(np.linalg.norm(A)*np.linalg.norm(B))"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsUZmDX0vWM6"
      },
      "source": [
        "def load_data(directory):\n",
        "\n",
        "  ############ Load training data\n",
        "\n",
        "  sources = []\n",
        "  references = []\n",
        "  candidates = []\n",
        "  scores = []\n",
        "  labels = []\n",
        "  targets = []\n",
        "\n",
        "  with open(f\"{directory}/train.txt\", encoding=\"utf-8\") as f:\n",
        "    reader = f.readlines()\n",
        "    for count,row in enumerate(reader):\n",
        "      if count % 6 == 0:\n",
        "        sources.append(row.strip('\\n'))\n",
        "      elif count % 6 == 1:\n",
        "        references.append( strip_punc(row.strip('\\n')) )  \n",
        "      elif count % 6 == 2:\n",
        "        candidates.append( strip_punc(row.strip('\\n')) ) \n",
        "      elif count % 6 == 3:\n",
        "        scores.append(row.strip('\\n'))\n",
        "      elif count % 6 == 4:\n",
        "        labels.append(row.strip('\\n'))\n",
        "        if (row.strip('\\n') == 'H'):\n",
        "          targets.append(1)\n",
        "        else:\n",
        "          targets.append(0)\n",
        "\n",
        "  dict = {\n",
        "      'source':sources,\n",
        "      'reference':references,\n",
        "      'candidate':candidates,\n",
        "      'bleu_uni':scores,\n",
        "      'label':labels,\n",
        "      'target':targets\n",
        "          }\n",
        "  train_data = pd.DataFrame(dict)\n",
        "\n",
        "  ############ Load test data\n",
        "\n",
        "  sources = []\n",
        "  references = []\n",
        "  candidates = []\n",
        "  scores = []\n",
        "  labels = []\n",
        "  targets = []\n",
        "\n",
        "  with open(f\"{directory}/test.txt\", encoding=\"utf-8\") as f:\n",
        "  \n",
        "    reader = f.readlines()\n",
        "    for count,row in enumerate(reader):\n",
        "      if count % 6 == 0:\n",
        "        sources.append(row.strip('\\n'))\n",
        "      elif count % 6 == 1:\n",
        "        references.append( strip_punc(row.strip('\\n')) )\n",
        "      elif count % 6 == 2:\n",
        "        candidates.append( strip_punc(row.strip('\\n')) )\n",
        "      elif count % 6 == 3:\n",
        "        scores.append(row.strip('\\n'))\n",
        "      elif count % 6 == 4:\n",
        "        labels.append(row.strip('\\n'))\n",
        "        if (row.strip('\\n') == 'H'):\n",
        "          targets.append(1)\n",
        "        else:\n",
        "          targets.append(0)\n",
        "    \n",
        "  dict = {\n",
        "      'source':sources,\n",
        "      'reference':references,\n",
        "      'candidate':candidates,\n",
        "      'bleu_uni':scores,\n",
        "      'label':labels,\n",
        "      'target':targets\n",
        "          }\n",
        "  test_data = pd.DataFrame(dict)\n",
        "\n",
        "  return train_data, test_data"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7jimxKiP5vS"
      },
      "source": [
        "\n",
        "############################################################\n",
        "############ Begin execution\n",
        "\n",
        "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiYDzHkXUQ-Q"
      },
      "source": [
        "############ Load data from files\n",
        "\n",
        "stopwords = set_stopwords()\n",
        "directory = '/content/drive/My Drive/translations'\n",
        "train_data, test_data = load_data(directory)\n",
        "\n",
        "############ Form custom features for train\n",
        "\n",
        "cos_s_r = []\n",
        "cos_s_c = []\n",
        "cos_c_r = []\n",
        "\n",
        "for i in range(len(train_data.index)):\n",
        "\n",
        "  row = train_data.loc[i]\n",
        "\n",
        "  text = row['source']\n",
        "  sour_vec = sbert_model.encode([text])[0]\n",
        "  sour_vectors.append(sour_vec)\n",
        "\n",
        "  text = process(row['reference'])\n",
        "  ref_vec = sbert_model.encode([text])[0]\n",
        "  ref_vectors.append(ref_vec)\n",
        "\n",
        "  text = process(row['candidate'])\n",
        "  can_vec = sbert_model.encode([text])[0]\n",
        "  can_vectors.append(can_vec)\n",
        "\n",
        "  cos_s_r.append( cosine_similarity(sour_vec, ref_vec) )\n",
        "  cos_s_c.append( cosine_similarity(sour_vec, can_vec) )\n",
        "  cos_c_r.append( cosine_similarity(can_vec, ref_vec) )\n",
        "\n",
        "train_data['cos_s_r'] = cos_s_r\n",
        "train_data['cos_s_c'] = cos_s_c\n",
        "train_data['cos_c_r'] = cos_c_r\n",
        "\n",
        "############ Form custom features for test\n",
        "\n",
        "cos_s_r = []\n",
        "cos_s_c = []\n",
        "cos_c_r = []\n",
        "\n",
        "for i in range(len(test_data.index)):\n",
        "\n",
        "  row = test_data.loc[i]\n",
        "\n",
        "  text = row['source']\n",
        "  sour_vec = sbert_model.encode([text])[0]\n",
        "  sour_vectors.append(sour_vec)\n",
        "\n",
        "  text = process(row['reference'])\n",
        "  ref_vec = sbert_model.encode([text])[0]\n",
        "  ref_vectors.append(ref_vec)\n",
        "\n",
        "  text = process(row['candidate'])\n",
        "  can_vec = sbert_model.encode([text])[0]\n",
        "  can_vectors.append(can_vec)\n",
        "\n",
        "  cos_s_r.append( cosine_similarity(sour_vec, ref_vec) )\n",
        "  cos_s_c.append( cosine_similarity(sour_vec, can_vec) )\n",
        "  cos_c_r.append( cosine_similarity(can_vec, ref_vec) )\n",
        "\n",
        "test_data['cos_s_r'] = cos_s_r\n",
        "test_data['cos_s_c'] = cos_s_c\n",
        "test_data['cos_c_r'] = cos_c_r"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sDFGaEVzVq8"
      },
      "source": [
        "smoother = SmoothingFunction().method4\n",
        "\n",
        "############ Compute MT Scores for Train \n",
        "\n",
        "bleu_sm = []\n",
        "met = []\n",
        "nist = []\n",
        "\n",
        "for i in range(len(train_data.index)):\n",
        "\n",
        "  row = train_data.loc[i]\n",
        "\n",
        "  ref_tokens = word_tokenize(row['reference'])\n",
        "  can_tokens = word_tokenize(row['candidate'])\n",
        "  bleu_sm.append( sentence_bleu([ref_tokens], can_tokens, smoothing_function=smoother) )\n",
        "  met.append( nltk.translate.meteor_score.meteor_score([row['reference']], row['candidate']) )\n",
        "  nist.append( sentence_nist([ref_tokens], can_tokens, 2) )\n",
        "\n",
        "train_data['bleu_sm'] = bleu_sm\n",
        "train_data['met'] = met\n",
        "train_data['nist'] = nist\n",
        "\n",
        "############ Compute MT Scores for Test\n",
        "\n",
        "bleu_sm = []\n",
        "met = []\n",
        "nist = []\n",
        "\n",
        "for i in range(len(test_data.index)):\n",
        "\n",
        "  row = test_data.loc[i]\n",
        "\n",
        "  ref_tokens = word_tokenize(row['reference'])\n",
        "  can_tokens = word_tokenize(row['candidate'])\n",
        "  bleu_sm.append( sentence_bleu([ref_tokens], can_tokens, smoothing_function=smoother) )\n",
        "  met.append( nltk.translate.meteor_score.meteor_score([row['reference']], row['candidate']) )\n",
        "  nist.append( sentence_nist([ref_tokens], can_tokens,2) )\n",
        "\n",
        "test_data['bleu_sm'] = bleu_sm\n",
        "test_data['met'] = met\n",
        "test_data['nist'] = nist"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1qh4xZYHacn"
      },
      "source": [
        "############ Evaluate various models on various feature selections\n",
        "\n",
        "col_set = []\n",
        "col_set.append(['bleu_uni'])\n",
        "col_set.append(['bleu_sm'])\n",
        "col_set.append(['met'])\n",
        "col_set.append(['nist'])\n",
        "col_set.append(['bleu_uni','bleu_sm','met','nist'])\n",
        "col_set.append(['cos_s_r'])\n",
        "col_set.append(['cos_s_c'])\n",
        "col_set.append(['cos_c_r'])\n",
        "col_set.append(['cos_s_r','cos_s_c','cos_c_r'])\n",
        "col_set.append(['bleu_uni','cos_s_r','cos_s_c','cos_c_r'])\n",
        "col_set.append(['bleu_uni','nist','cos_s_r','cos_s_c','cos_c_r'])\n",
        "col_set.append(['bleu_uni','bleu_sm','met','nist','cos_s_r','cos_s_c','cos_c_r'])\n",
        "col_set.append(['bleu_uni','nist','cos_c_r'])\n",
        "\n",
        "result_data = []\n",
        "\n",
        "with open(\"output.csv\", \"w\") as f:\n",
        "\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(['features','model','f1','accuracy'])\n",
        "\n",
        "  for col in col_set:\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    X_train = build_matrix(col,train_data).astype('float32')\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    X_test = build_matrix(col,test_data).astype('float32')\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    y_train = np.asarray(train_data['target'])\n",
        "    y_test = np.asarray(test_data['target'])\n",
        "\n",
        "    ###### SVC Linear\n",
        "\n",
        "    model_parameters = {\n",
        "      'kernel': ['linear'],\n",
        "        'C': [0.1, 0.5, 1, 5, 10, 50, 100]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(SVC(), model_parameters, cv=5, scoring=\"f1\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    accuracy = 1-(np.mean( np.abs(y_pred - y_test) ) )\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result_data.append([(' ').join(col),'SVC linear',f1,accuracy])\n",
        "    writer.writerow(result_data[-1])\n",
        "\n",
        "    ###### SVC RBF\n",
        "\n",
        "    model_parameters = {\n",
        "      'kernel': ['rbf'],\n",
        "        'C': [0.1, 0.5, 1, 5, 10, 50, 100],\n",
        "        'gamma': [0.1, 0.5, 1, 3, 6, 10],\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(SVC(), model_parameters, cv=5, scoring=\"f1\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    accuracy = 1-(np.mean( np.abs(y_pred - y_test) ) )\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result_data.append([(' ').join(col),'SVC RBF',f1,accuracy])\n",
        "    writer.writerow(result_data[-1])\n",
        "\n",
        "    ###### Logistic Regression\n",
        "\n",
        "    model_parameters = {\n",
        "        'C': [0.1, 0.5, 1, 5, 10, 50, 100],\n",
        "        'max_iter': [1000]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(LogisticRegression(), model_parameters, cv=5, scoring=\"f1\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    accuracy = 1-(np.mean( np.abs(y_pred - y_test) ) )\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result_data.append([(' ').join(col),'logistic regression',f1,accuracy])\n",
        "    writer.writerow(result_data[-1])\n",
        "\n",
        "    ##### KNN\n",
        "\n",
        "    model_parameters = {\n",
        "      'algorithm': ['auto'],\n",
        "        'n_neighbors': range(1, 51),\n",
        "        'leaf_size': range(5, 61, 5)\n",
        "    }\n",
        "    grid_search = GridSearchCV(KNeighborsClassifier(), model_parameters, cv=5, scoring=\"f1\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    accuracy = 1-(np.mean( np.abs(y_pred - y_test) ) )\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result_data.append([(' ').join(col),'KNN',f1,accuracy])\n",
        "    writer.writerow(result_data[-1])\n",
        "\n",
        "    ##### Decision Tree\n",
        "\n",
        "    model_parameters = {\n",
        "        'max_depth': range(1, 51),\n",
        "        'min_samples_split': range(2, 11)\n",
        "    }\n",
        "    grid_search = GridSearchCV(DecisionTreeClassifier(), model_parameters, cv=5, scoring=\"f1\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    accuracy = 1-(np.mean( np.abs(y_pred - y_test) ) )\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result_data.append([(' ').join(col),'Decision Tree',f1,accuracy])\n",
        "    writer.writerow(result_data[-1])\n",
        "\n",
        "    ##### MLP\n",
        "\n",
        "    model_parameters = {\n",
        "        'solver': ['lbfgs'], \n",
        "        'max_iter': [750,1500], \n",
        "        'alpha': 10.0 ** -np.arange(1, 7), \n",
        "        'hidden_layer_sizes':np.arange(12, 20), \n",
        "        'random_state':[0,4,8]\n",
        "        }\n",
        "    \n",
        "    grid_search = GridSearchCV(neural_network.MLPClassifier(), model_parameters, n_jobs=-1, scoring=\"f1\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    accuracy = 1-(np.mean( np.abs(y_pred - y_test) ) )\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result_data.append([(' ').join(col),'MLP',f1,accuracy])\n",
        "    writer.writerow(result_data[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoXca89az-O7"
      },
      "source": [
        "############ Output summary CSV\n",
        "\n",
        "results = dict()\n",
        "\n",
        "for result in result_data:\n",
        "  features = result[0]\n",
        "  model = result[1]\n",
        "  f1 = result[2]\n",
        "  acc = result[3]\n",
        "\n",
        "  if features not in results:\n",
        "    results[features] = dict()\n",
        "    results[features]['best_f1_model'] = ''\n",
        "    results[features]['best_f1_score'] = 0.0\n",
        "    results[features]['average_f1_score'] = 0.0\n",
        "    results[features]['average_acc_score'] = 0.0\n",
        "    results[features]['total_seen'] = 0\n",
        "    results[features]['total_f1_score'] = 0.0\n",
        "    results[features]['total_acc_score'] = 0.0\n",
        "\n",
        "  results[features]['total_seen'] += 1\n",
        "  results[features]['total_f1_score'] += f1\n",
        "  results[features]['total_acc_score'] += acc\n",
        "  results[features]['average_f1_score'] = (results[features]['total_f1_score']/results[features]['total_seen'])\n",
        "  results[features]['average_acc_score'] = (results[features]['total_acc_score']/results[features]['total_seen'])\n",
        "\n",
        "  if f1 > results[features]['best_f1_score']:\n",
        "    results[features]['best_f1_score'] = f1\n",
        "    results[features]['best_f1_model'] = model\n",
        "\n",
        "with open(\"output_agg.csv\", \"w\") as f:\n",
        "\n",
        "  writer = csv.writer(f)\n",
        "\n",
        "  key1 = list(results.keys())[0]\n",
        "  col_names = list(results[features].keys())\n",
        "  col_names.insert(0,'features')\n",
        "  writer.writerow(col_names[0:5])\n",
        "  \n",
        "  for features in results.keys():\n",
        "\n",
        "    nums = list(results[features].values())\n",
        "    nums.insert(0,features)\n",
        "    writer.writerow(nums[0:5])\n"
      ],
      "execution_count": 109,
      "outputs": []
    }
  ]
}